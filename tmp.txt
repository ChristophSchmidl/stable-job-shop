************************************
            Plotting
************************************
- https://www.reddit.com/r/reinforcementlearning/comments/gnvlcp/way_to_plot_goodlooking_rewards_plots/
- https://spinningup.openai.com/en/latest/spinningup/bench.html
- https://stackabuse.com/seaborn-line-plot-tutorial-and-examples/
- https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html
- https://stable-baselines.readthedocs.io/en/master/guide/examples.html
- https://stable-baselines.readthedocs.io/en/master/guide/callbacks.html#callbacks
- Deep Reinforcement Learning that Matters: https://arxiv.org/pdf/1709.06560.pdf
- https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/bench/monitor.py#L13
- https://github.com/araffin/rl-tutorial-jnrr19
- https://neurogym.github.io/example_neurogym_rl.html
- https://chowdera.com/2022/04/202204130435468291.html
- https://chowdera.com/2022/03/202203271932500916.html

************************************
            Logging
************************************
- https://www.youtube.com/watch?v=w8QHoVam1-I&ab_channel=Jayanam
- https://stackoverflow.com/questions/69181347/stable-baselines3-log-rewards
- https://stable-baselines3.readthedocs.io/en/master/common/logger.html
- https://www.reddit.com/r/reinforcementlearning/comments/ofm0bu/comment/h4dekga/
- https://github.com/hill-a/stable-baselines/issues/1139
- https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html
- https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb#scrollTo=pUWGZp3i9wyf

************************************
            Blogs
************************************
- https://araffin.github.io/post/rliable/
- https://openlab-flowers.inria.fr/t/how-many-random-seeds-should-i-use-statistical-power-analysis-in-deep-reinforcement-learning-experiments/457



************************************
            TODO
************************************

- Finish TensorboardCallback
    - Plot makespan (per episode)
    - Plot reward/return (per episode)
    - Plot average return (per episode, different seeds?)

- Finish ProgressBarCallback
    - Add a flag for switching between episodes and timesteps
    - 
- Question: Is model.learn() resetting the weights of the model each time it gets executed?


************************************
    Wandb and stable-baselines3
************************************

- ML Frameworks: Stable-Baselines3 w/ Antonin Raffin and Anssi Kanervisto: https://www.youtube.com/watch?v=ed1bqaZGOQw
- https://wandb.ai/site/articles/running-hyperparameter-sweeps-to-pick-the-best-model-using-w-b
- https://wandb.ai/jmcginn/pytorch-nflows-EOS/reports/Hyperparameter-Optimisation--VmlldzoxMDEwMDc2
- Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W&B.ipynb: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=2Q-xqt5ZHm98
- Log_(Almost)_Anything_with_W&B_Media.ipynb: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb
- Simple_PyTorch_Integration.ipynb: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb#scrollTo=XUvF9RacdJ05
- https://docs.wandb.ai/guides/sweeps/existing-project
- https://wandb.ai/site/articles/hyperparameter-tuning-as-easy-as-1-2-3
- https://docs.wandb.ai/guides/integrations/pytorch
- https://docs.wandb.ai/guides/integrations/other/stable-baselines-3
- https://docs.wandb.ai/guides/sweeps/quickstart
- https://medium.com/analytics-vidhya/weights-and-biases-ify-stable-baselines-models-in-finrl-f11b67f2a6a7
- https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html
- https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe
- https://github.com/wandb/examples/blob/master/colabs/stable_baselines3/Stable_Baselines3_wandb_experiment_tracking.ipynb


************************************
    Callbacks in stable-baselines
************************************

for key, value in self.locals.items():
    print(key)

self
total_timesteps
callback
log_interval
eval_env
eval_freq
n_eval_episodes
tb_log_name
eval_log_path
reset_num_timesteps
use_masking
iteration
env
rollout_buffer
n_rollout_steps
n_steps
action_masks
obs_tensor
actions
values
log_probs
new_obs
rewards
dones
infos


************************************
        Paper hyperparameters
************************************

clip_param: 0.5653 -> SB3 PPO: clip_range
entropy_end: 0.00221
entropy_start: 0.005503
kl_coeff: 0.116
kl_target: 0.08849 -> SB3 PPO: target_kl
layer_size: 264
lr_end: 0.00009277 -> SB3 PPO: learning_rate     
lr_start: 0.0008534
num_sgd_iter: 12 -> SB3 PPO: n_epochs?
vf_clip_param: 24 -> SB3 PPO: clip_range_vf
vf_loss_coeff: 0.9991 -> SB3 PPO: vf_coef
episode_reward_mean: 179.046